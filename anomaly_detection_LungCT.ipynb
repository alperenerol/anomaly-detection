{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6209c93-8c5b-46de-be2a-4695d56985da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alperen Erol - 200051583 - Unsupervised Anomaly Detection on Medical Images project notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ab0a39-d8e3-4cc8-a737-db79a6fbf269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss, L1Loss\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "\n",
    "from monai import transforms\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.utils import first, set_determinism\n",
    "\n",
    "from GenerativeModels.generative.inferers import VQVAETransformerInferer\n",
    "from GenerativeModels.generative.networks.nets import VQVAE, DecoderOnlyTransformer\n",
    "from GenerativeModels.generative.utils.enums import OrderingType\n",
    "from GenerativeModels.generative.utils.ordering import Ordering\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb47416-70dc-4673-ab78-d4c77dd3bb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-max normalisation function\n",
    "def min_max_normalize(tensor):\n",
    "    tensor_min = tensor.min()\n",
    "    tensor_max = tensor.max()\n",
    "    normalized_tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n",
    "    return normalized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b8ec2d-6ab4-4e09-ba9a-5c38e37ff34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class to load .npz files (image-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f962fbf8-b8c7-45f8-98a0-716ead09890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPZDataset(Dataset):\n",
    "    def __init__(self, root_dir, indices=None, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.files = [f for f in os.listdir(root_dir) if f.endswith('.npz')]\n",
    "        if indices is not None:\n",
    "            self.files = [self.files[i] for i in indices]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.root_dir, self.files[idx])\n",
    "        data = np.load(file_path)\n",
    "        sample = data[\"image\"]\n",
    "        \n",
    "        # image data\n",
    "        # Adds channel dimension\n",
    "        sample = np.expand_dims(sample, axis=0)\n",
    "\n",
    "        # Adds batch dimension and resizes to 256x256 using bicubic interpolation\n",
    "        sample = torch.tensor(sample, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "        sample = F.interpolate(sample, size=(128, 128), mode='bicubic', align_corners=False)\n",
    "        \n",
    "        # Normalize the sample\n",
    "        sample = min_max_normalize(sample) \n",
    "        \n",
    "        sample = sample.squeeze(0)  # Removes batch dimension\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample.clone().detach().to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfca42b0-8022-4f2f-bbba-e65cdfc87790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates and loads of training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8fa366-1369-4e2d-b537-a42a77a93a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"normal_slices_training\"\n",
    "\n",
    "train_data = NPZDataset(root_dir, indices=list(range(12000)))\n",
    "val_data_normal = NPZDataset(root_dir=\"normal_slices_training\", indices=list(range(12000, 14000)))\n",
    "val_data = val_data_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eadb49e-0a85-4c63-991c-30dd2254c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071133bb-7325-48af-92b2-0cf0342ba4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data normalised\n",
    "\n",
    "check_data = next(iter(val_loader))[0]\n",
    "\n",
    "print(\"This is minimum value of Sample tensor:\",torch.min(check_data))\n",
    "print(\"This is maximum value of Sample tensor:\",torch.max(check_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f4e02-a1ff-47be-98b6-7677c9d0566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3 examples from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ee9de-628b-4c4c-98a2-bf70cad01f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data = next(iter(train_loader))\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3)  # Added figsize for larger images\n",
    "for image_n in range(3):\n",
    "    ax[image_n].imshow(check_data[image_n].squeeze(), cmap=\"gray\")  # Squeeze out the channel dimension\n",
    "    ax[image_n].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b6414-d851-4a04-8268-9543df825acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQ-VAE : Define network, optimizer and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcffc49-d680-47c0-b2d8-c9dab8863400",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "vqvae_model = VQVAE(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_res_layers=2,\n",
    "    downsample_parameters=((2, 4, 1, 1), (2, 4, 1, 1)),\n",
    "    upsample_parameters=((2, 4, 1, 1, 0), (2, 4, 1, 1, 0)),\n",
    "    num_channels=(256, 256),\n",
    "    num_res_channels=(256, 256),\n",
    "    num_embeddings=16,\n",
    "    embedding_dim=64,\n",
    ")\n",
    "vqvae_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18276c-f7c3-4018-93b6-ffbbdc1ebe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQ-VAE model total parameters\n",
    "total_params = sum(p.numel() for p in vqvae_model.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6bb874-3729-41a0-afbf-cabf6d220641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQ-VAE Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ea833-1370-4260-8cc4-154765fc6845",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=vqvae_model.parameters(), lr=5e-4) # Optimiser and learning rate\n",
    "l1_loss = L1Loss()\n",
    "n_epochs = 100 # training epoch adjusted here\n",
    "val_interval = 10 # validation interval\n",
    "epoch_losses = []\n",
    "val_epoch_losses = []\n",
    "\n",
    "#Early stopping params\n",
    "patience = 10  # Number of epochs with no improvement to wait before stopping\n",
    "best_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    vqvae_model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=110)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "        images = batch.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # model outputs reconstruction and the quantization error\n",
    "        reconstruction, quantization_loss = vqvae_model(images=images)\n",
    "        recons_loss = l1_loss(reconstruction.float(), images.float())\n",
    "        loss = recons_loss + quantization_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += recons_loss.item()\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            {\"recons_loss\": epoch_loss / (step + 1), \"quantization_loss\": quantization_loss.item() / (step + 1)}\n",
    "        )\n",
    "    epoch_losses.append(epoch_loss / (step + 1))\n",
    "    \n",
    "\n",
    "    # Validation\n",
    "    vqvae_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for val_step, batch_valid in enumerate(val_loader, start=1):\n",
    "            images_valid = batch_valid.to(device)\n",
    "            reconstruction, quantization_loss = vqvae_model(images=images_valid)\n",
    "            recons_loss = l1_loss(reconstruction.float(), images_valid.float())\n",
    "            val_loss += recons_loss.item()\n",
    "\n",
    "    val_loss /= val_step\n",
    "    val_epoch_losses.append(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        torch.save(vqvae_model.state_dict(), f\"demo_vqvae_training_epoch_{epoch}.pth\")\n",
    "        \n",
    "        # At the end of each epoch original/reconstruction image plot from validation set\n",
    "        with torch.no_grad():\n",
    "            sample_images = images_valid[:3]  # Taking first 3 images from the last batch\n",
    "            reconstructions = vqvae_model(sample_images)[0]  # Getting reconstructions\n",
    "\n",
    "            fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 8))\n",
    "            for i in range(3):\n",
    "                axes[0, i].imshow(sample_images[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "                axes[0, i].set_title(f\"Original {i+1}\")\n",
    "                axes[0, i].axis('off')\n",
    "\n",
    "                axes[1, i].imshow(reconstructions[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "                axes[1, i].set_title(f\"Reconstruction {i+1}\")\n",
    "                axes[1, i].axis('off')\n",
    "            plt.show()\n",
    "        \n",
    "        # After computing validation loss:\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"train completed, total time: {total_time}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c98bb7-2040-447a-851d-4465d5b3774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Training and Validation Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa005757-6435-4f50-ba13-632ce7abf550",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(epoch_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, epoch_losses, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_epoch_losses, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c78b35-8aea-4bfb-b306-688a3037377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained VQ-VAE model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddd0eb4-9b78-4fc6-b1e2-794db20cac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved state dictionary of pre-trained VQ-VAE model\n",
    "saved_state_dict = torch.load(\"vqvae_training5_epoch_99.pth\")  # Replace with the actual path\n",
    "\n",
    "# Load the state dictionary into the VQ-VAE model\n",
    "vqvae_model.load_state_dict(saved_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8408aea4-d8e2-470c-9499-793650221766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Average SSIM of Normal Images Only Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8419bb62-d1db-4dfc-a58a-8b5ef63ce5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "val_loader_ssim = DataLoader(val_data, batch_size=1, shuffle=True, num_workers=4) # Batch-size defined as 1 for easy process\n",
    "total_ssim = 0\n",
    "total_images = 0\n",
    "vqvae_model.eval()\n",
    "\n",
    "with torch.no_grad():  # No need to track gradients\n",
    "    for data in val_loader_ssim:\n",
    "        images = data.to(device)  # Adjust based on your dataset structure\n",
    "        original_images = images.squeeze().cpu().numpy()\n",
    "        reconstructed_images = vqvae_model(images)[0].squeeze().cpu().numpy()\n",
    "        #plt.imshow(reconstructed_images)\n",
    "        #print(reconstructed_images)\n",
    "        for original, reconstructed in zip(original_images, reconstructed_images):\n",
    "            # If images have multiple channels, convert them to grayscale or calculate SSIM per channel\n",
    "            ssim_value = ssim(original, reconstructed, data_range=1)  # Using the first channel (if multiple channels)\n",
    "            total_ssim += ssim_value\n",
    "            total_images += 1\n",
    "\n",
    "average_ssim = total_ssim / total_images\n",
    "print(f'Average SSIM: {average_ssim:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582dabcd-1b0a-407c-9575-8dbcad352ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE(L1 loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6304baf-de84-4433-b126-b8eb9384e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_loss = L1Loss()\n",
    "vqvae_model.eval()\n",
    "val_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_step, batch_valid in enumerate(val_loader, start=1):\n",
    "        images_valid = batch_valid.to(device)\n",
    "        reconstruction, quantization_loss = vqvae_model(images=images_valid)\n",
    "        recons_loss = l1_loss(reconstruction.float(), images_valid.float())\n",
    "        val_loss += recons_loss.item()\n",
    "\n",
    "val_loss /= val_step\n",
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b12c18-bbdf-4450-8503-b0470fae6649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection of 2D latent representation to 1D sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91add07-3053-43c9-938d-d7b6a9b1dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spatial dimensions of data\n",
    "\n",
    "test_data = next(iter(train_loader)).to(device)\n",
    "spatial_shape = vqvae_model.encode_stage_2_inputs(test_data).shape[2:] # quantizations\n",
    "\n",
    "# Initialize an Ordering class that projects a 2D image into a 1D sequence.\n",
    "ordering = Ordering(ordering_type=OrderingType.RASTER_SCAN.value, spatial_dims=2, dimensions=(1,) + spatial_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee3f300-33aa-42a9-8426-5ea5fa917050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define auto-regressive transformer network, VQ-VAE/Transformer inferer, optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e391c-b307-45d5-adbc-944832ca2605",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transformer_model = DecoderOnlyTransformer(\n",
    "    num_tokens=16 + 1,\n",
    "    max_seq_len=spatial_shape[0] * spatial_shape[1],\n",
    "    attn_layers_dim=128,\n",
    "    attn_layers_depth=16,\n",
    "    attn_layers_heads=16,\n",
    ")\n",
    "transformer_model.to(device)\n",
    "\n",
    "inferer = VQVAETransformerInferer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d37719-192b-4937-883c-22b1c798b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and Loss function of transformer\n",
    "optimizer = torch.optim.Adam(params=transformer_model.parameters(), lr=5e-3)\n",
    "ce_loss = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a00fdce-b106-4330-9a92-94e993f97ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ee0d41-fed5-4be3-bfe7-8a894cbd4138",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 150 #training epoch adjusted here\n",
    "val_interval = 10 #validation interval\n",
    "epoch_losses = []\n",
    "val_epoch_losses = []\n",
    "vqvae_model.eval()\n",
    "\n",
    "#Early stopping params\n",
    "patience = 10  # Number of epochs with no improvement to wait before stopping\n",
    "best_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    transformer_model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=110)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "\n",
    "        images = batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        logits, target, _ = inferer(images, vqvae_model, transformer_model, ordering, return_latent=True)\n",
    "        logits = logits.transpose(1, 2)\n",
    "\n",
    "        loss = ce_loss(logits, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix({\"ce_loss\": epoch_loss / (step + 1)})\n",
    "    epoch_losses.append(epoch_loss / (step + 1))\n",
    "    \n",
    "    #Validation\n",
    "    transformer_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for val_step, batch in enumerate(val_loader, start=1):\n",
    "\n",
    "            images = batch.to(device)\n",
    "\n",
    "            logits, quantizations_target, _ = inferer(\n",
    "                images, vqvae_model, transformer_model, ordering, return_latent=True\n",
    "            )\n",
    "            logits = logits.transpose(1, 2)\n",
    "\n",
    "            loss = ce_loss(logits[:, :, :-1], quantizations_target[:, 1:])\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        torch.save(transformer_model.state_dict(), f\"demo_transformer_model_training_epoch_{epoch}.pth\")\n",
    "        \n",
    "        # get and show sample images generated by transformer\n",
    "        sample = inferer.sample(\n",
    "            vqvae_model=vqvae_model,\n",
    "            transformer_model=transformer_model,\n",
    "            ordering=ordering,\n",
    "            latent_spatial_dim=(spatial_shape[0], spatial_shape[1]),\n",
    "            starting_tokens=vqvae_model.num_embeddings * torch.ones((1, 1), device=device),\n",
    "        )\n",
    "        plt.imshow(sample[0, 0, ...].cpu().detach())\n",
    "        plt.title(f\"Sample epoch {epoch}\")\n",
    "        plt.show()\n",
    "        val_loss /= val_step\n",
    "        val_epoch_losses.append(val_loss)\n",
    "        val_loss /= val_step\n",
    "        val_epoch_losses.append(val_loss)\n",
    "        \n",
    "        # After computing validation loss:\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"train completed, total time: {total_time}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd805d7-91eb-4228-8ff5-577cd5beeb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964c1fd5-c676-4ecc-920e-cfffb7414eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved state dictionary\n",
    "saved_state_dict = torch.load(\"transformer_model_training_25092023_epoch_99.pth\")  # Replace with the actual path\n",
    "\n",
    "# Load the state dictionary into the VQ-VAE model\n",
    "transformer_model.load_state_dict(saved_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c0eac-3900-4feb-aa52-542b05c0b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch models to evaluation mode\n",
    "vqvae_model.eval()\n",
    "transformer_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1579d814-83fa-4809-b119-9c8dda627803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample image from pre-trained Transformer model\n",
    "sample = inferer.sample(\n",
    "    vqvae_model=vqvae_model,\n",
    "    transformer_model=transformer_model,\n",
    "    ordering=ordering,\n",
    "    latent_spatial_dim=(spatial_shape[0], spatial_shape[1]),\n",
    "    starting_tokens=vqvae_model.num_embeddings * torch.ones((1, 1), device=device),\n",
    ")\n",
    "plt.imshow(sample[0, 0, ...].cpu().detach())\n",
    "#plt.title(f\"Sample epoch {epoch}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b4df85-9431-4ed0-a360-004753b3c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image-wise anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e9c94-ec6e-4cbe-be3b-3312dcb51ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load normal and abnormal test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781cff74-f3c5-4671-a86a-469929de8994",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae_model.eval()\n",
    "transformer_model.eval()\n",
    "\n",
    "test_data_normal = NPZDataset(root_dir=\"normal_slices_training\", indices=list(range(14000, 16000)))\n",
    "test_data_normal_loader = DataLoader(test_data_normal, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "test_data_abnormal = NPZDataset(root_dir=\"abnormal_slices\", indices=list(range(1600)))\n",
    "test_data_abnormal_loader = DataLoader(test_data_abnormal, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7a3d22-02ff-48e7-a124-edef190f95f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get normal distribution log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b95c555-6837-4771-b005-7ac3bd9b81d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_likelihoods = []\n",
    "\n",
    "progress_bar = tqdm(enumerate(test_data_normal_loader), total=len(test_data_normal_loader), ncols=110)\n",
    "progress_bar.set_description(f\"Normal-distribution data\")\n",
    "for step, batch in progress_bar:\n",
    "    images = batch.to(device)\n",
    "\n",
    "    log_likelihood = inferer.get_likelihood(\n",
    "        inputs=images, vqvae_model=vqvae_model, transformer_model=transformer_model, ordering=ordering\n",
    "    )\n",
    "    normal_likelihoods.append(log_likelihood.sum(dim=(1, 2)).cpu().numpy())\n",
    "\n",
    "normal_likelihoods = np.concatenate(normal_likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f689220f-f4de-4581-9589-2f3a23840bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get abnormal distribution log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fda007-cb90-41ef-9487-54e5b2fba8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_likelihoods = []\n",
    "\n",
    "progress_bar = tqdm(enumerate(test_data_abnormal_loader), total=len(test_data_abnormal_loader), ncols=110)\n",
    "progress_bar.set_description(f\"Abnormal-distribution data\")\n",
    "for step, batch in progress_bar:\n",
    "    images = batch.to(device)\n",
    "\n",
    "    log_likelihood = inferer.get_likelihood(\n",
    "        inputs=images, vqvae_model=vqvae_model, transformer_model=transformer_model, ordering=ordering\n",
    "    )\n",
    "    abnormal_likelihoods.append(log_likelihood.sum(dim=(1, 2)).cpu().numpy())\n",
    "\n",
    "abnormal_likelihoods = np.concatenate(abnormal_likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155821f8-fdc1-49c5-8d92-7fd05b84b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal and Abnormal Log-likelihood plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e56815-a2e2-4a41-bf93-4d0847c6f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\", {\"axes.grid\": False})\n",
    "sns.kdeplot(normal_likelihoods, bw_adjust=1, label=\"Normal-distribution\", fill=True, cut=True)\n",
    "sns.kdeplot(abnormal_likelihoods, bw_adjust=1, label=\"Abnormal-distribution\", cut=True, fill=True)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Log-likelihood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b4afc-a548-488f-865f-74a14859efe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC-ROC score, True Positive Rate, False Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa93c9fc-6de2-41a7-a926-b3a4e4025a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# log-likelihood values for two classes\n",
    "log_likelihood_class0 = abnormal_likelihoods\n",
    "log_likelihood_class1 = normal_likelihoods\n",
    "\n",
    "# Determination of threshold\n",
    "threshold = -1515.0 # intersection point observed from the log-likelihood plot\n",
    "\n",
    "# Combine the log-likelihood values and labels\n",
    "log_likelihood = np.concatenate((log_likelihood_class0, log_likelihood_class1))\n",
    "likelihood_labels = np.concatenate((np.zeros_like(log_likelihood_class0), np.ones_like(log_likelihood_class1)))\n",
    "\n",
    "# Classify observations based on the threshold\n",
    "predicted = (log_likelihood > threshold).astype(int)\n",
    "\n",
    "# Calculate the TPR and FPR\n",
    "fpr, tpr, _ = roc_curve(likelihood_labels, predicted)\n",
    "\n",
    "# Compute the AUC-ROC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(f\"True Positive Rate: {tpr}\")\n",
    "print(f\"False Positive Rate: {fpr}\")\n",
    "print(f\"AUC-ROC: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b6670-d67a-4041-a6c6-4761fac167d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf19601-b355-43f1-b835-95717f9662ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc328b28-511d-487c-a242-8115153d2665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720b115-7b54-45ab-b0f4-582c6c617a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b4f240-a61c-4b90-bd03-a725918a2eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Localised anomaly detection,segmentation and healing on a synthetic abnormal image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c8e8a3-8e7d-485a-930c-3b88c8905614",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = first(test_data_normal_loader)\n",
    "image_clean = input_image[0, ...]\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_clean[0, ...], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Clean image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b3a9b7-0eef-49b1-b97f-445a7600e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_corrupted = image_clean.clone()\n",
    "image_corrupted[0, 50:80, 40:60] = 1\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_corrupted[0, ...], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Corrupted image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd7ad2-bf41-4b27-91b2-b378187f2995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the log-likelihood and convert into a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c62cad4-2feb-4c4b-b570-0b5ba3435f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = inferer.get_likelihood(\n",
    "    inputs=image_corrupted[None, ...].to(device),\n",
    "    vqvae_model=vqvae_model,\n",
    "    transformer_model=transformer_model,\n",
    "    ordering=ordering,\n",
    ")\n",
    "likelihood = torch.exp(log_likelihood)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(likelihood.cpu()[0, ...])\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Log-likelihood\")\n",
    "plt.subplot(1, 2, 2)\n",
    "mask = log_likelihood.cpu()[0, ...] < torch.quantile(log_likelihood, 0.04).item()\n",
    "# Further mask with the healing mask\n",
    "resizer = torch.nn.Upsample(size=(128, 128), mode=\"nearest\")\n",
    "mask_upsampled = resizer(mask[None, None, ...].float()).int().squeeze()\n",
    "plt.imshow(mask_upsampled)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Healing mask\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e02ff4c-fc82-46c1-a24f-c351693b6436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this mask and the trained transformer to 'heal' the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3862aa2a-ea81-4a99-a529-c78f12431c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the mask\n",
    "mask_flattened = mask.reshape(-1)\n",
    "mask_flattened = mask_flattened[ordering.get_sequence_ordering()]\n",
    "\n",
    "latent = vqvae_model.index_quantize(image_corrupted[None, ...].to(device))\n",
    "latent = latent.reshape(latent.shape[0], -1)\n",
    "latent = latent[:, ordering.get_sequence_ordering()]\n",
    "latent = F.pad(latent, (1, 0), \"constant\", vqvae_model.num_embeddings)\n",
    "latent = latent.long()\n",
    "latent_healed = latent.clone()\n",
    "\n",
    "# heal the sequence\n",
    "# loop over tokens\n",
    "for i in range(1, latent.shape[1]):\n",
    "    if mask_flattened[i - 1]:\n",
    "        # if token is low probability, replace with tranformer's most likely token\n",
    "        logits = transformer_model(latent_healed[:, :i])\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # don't sample beginning of sequence token\n",
    "        probs[:, :, vqvae_model.num_embeddings] = 0\n",
    "        index = torch.argmax(probs[0, -1, :])\n",
    "        latent_healed[:, i] = index\n",
    "\n",
    "\n",
    "# reconstruct\n",
    "latent_healed = latent_healed[:, 1:]\n",
    "latent_healed = latent_healed[:, ordering.get_revert_sequence_ordering()]\n",
    "latent_healed = latent_healed.reshape((32, 32))\n",
    "\n",
    "image_healed = vqvae_model.decode_samples(latent_healed[None, ...]).cpu().detach()\n",
    "plt.imshow(image_healed[0, 0, ...], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Healed image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104959a1-5ecb-49bd-9251-b594b2313f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create anomaly maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e545e3-3400-4de9-a53d-e31b73b188ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a naive anomaly map using the difference\n",
    "difference_map = torch.abs(image_healed[0, 0, ...] - image_corrupted[0, ...])\n",
    "\n",
    "# Further mask with the healing mask\n",
    "resizer = torch.nn.Upsample(size=(128, 128), mode=\"nearest\")\n",
    "mask_upsampled = resizer(mask[None, None, ...].float()).int()\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(14, 8))\n",
    "plt.subplot(1, 5, 1)\n",
    "plt.imshow(image_clean[0, ...], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Clean image\")\n",
    "#image_corrupted = image_clean.clone()\n",
    "#image_corrupted[0, 25:40, 40:50] = 1\n",
    "plt.subplot(1, 5, 2)\n",
    "plt.imshow(image_corrupted[0, ...], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Corrupted image\")\n",
    "plt.subplot(1, 5, 3)\n",
    "plt.imshow(image_corrupted[0, ...] - image_clean[0, ...], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Ground-Truth anomaly mask\")\n",
    "plt.subplot(1, 5, 4)\n",
    "plt.imshow(mask_upsampled[0, 0, ...] * difference_map, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Predicted anomaly mask\")\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1, 5, 5)\n",
    "plt.imshow(image_healed[0, 0, ...], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Healed image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d435c-e7b6-4e93-8bda-92c878d3d982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1edfc9-0d95-45cd-813a-be674bd60941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220acde-f27d-47de-9154-053112d4d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice score calculation function\n",
    "def dice_score(predicted, target, epsilon=1e-7):\n",
    "    #predicted = predicted.view(-1).float()\n",
    "    #target = target.view(-1).float()\n",
    "    intersection = (predicted * target).sum()\n",
    "    return (2. * intersection + epsilon) / (predicted.sum() + target.sum() + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f6479-f343-44b4-8595-c15f2228da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic anomalies dice score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793d8eda-2d03-4ab3-9566-051ba9f09450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader should yield normal images\n",
    "\n",
    "dice_scores = []  # List to store dice scores for all test samples\n",
    "\n",
    "# Create a synthetic anomaly mask\n",
    "gt_mask = np.zeros((128, 128), dtype=float)\n",
    "gt_mask[50:80, 40:60] = 1.0\n",
    "#gt_mask = gt_mask.to(device)\n",
    "        \n",
    "# Loop over the test dataset\n",
    "for batch in test_data_normal_loader:\n",
    "    images = batch  \n",
    "    \n",
    "    # Loop over the batch\n",
    "    for i in range(len(images)):\n",
    "        image_clean = images[i].to(device)  # send image to device\n",
    "        \n",
    "        # create synthetic anomalies and synthetic gt mask\n",
    "        image_corrupted = image_clean.clone()\n",
    "        image_corrupted[0, 50:80, 40:60] = 1\n",
    "        \n",
    "        # ... Your code to calculate predicted_mask for the image\n",
    "        # This will use the code you showed above to calculate the mask based on log-likelihood\n",
    "        log_likelihood = inferer.get_likelihood(\n",
    "            inputs=image_corrupted[None, ...].to(device),\n",
    "            vqvae_model=vqvae_model,\n",
    "            transformer_model=transformer_model,\n",
    "            ordering=ordering)\n",
    "        likelihood = torch.exp(log_likelihood)\n",
    "        mask = log_likelihood.cpu()[0, ...] < torch.quantile(log_likelihood, 0.04).item()\n",
    "        # Further mask with the healing mask\n",
    "        resizer = torch.nn.Upsample(size=(128, 128), mode=\"nearest\")\n",
    "        predicted_mask = resizer(mask[None, None, ...].float()).squeeze()\n",
    "        #print(predicted_mask)\n",
    "        # Now calculate the dice score for this image\n",
    "        score = dice_score(predicted_mask, gt_mask)\n",
    "        dice_scores.append(score.item())\n",
    "\n",
    "# Now calculate the average dice score for the entire test dataset\n",
    "average_dice_score = sum(dice_scores) / len(dice_scores)\n",
    "\n",
    "print(f'Average Dice Score on Test Dataset: {average_dice_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b52690c-2825-4f87-9303-97889236ec9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557f44e-5822-47ed-b929-302e3104440f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b9be5f-3c38-4b12-ae83-2a10b95e98b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPZDataset2(Dataset):\n",
    "    def __init__(self, root_dir, indices=None, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.files = [f for f in os.listdir(root_dir) if f.endswith('.npz')]\n",
    "        if indices is not None:\n",
    "            self.files = [self.files[i] for i in indices]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.root_dir, self.files[idx])\n",
    "        data = np.load(file_path)\n",
    "        sample = data[\"image\"]\n",
    "        \n",
    "        # image data\n",
    "        # Adds channel dimension\n",
    "        sample = np.expand_dims(sample, axis=0)\n",
    "\n",
    "        # Adds batch dimension and resizes to 256x256 using bicubic interpolation\n",
    "        sample = torch.tensor(sample, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "        sample = F.interpolate(sample, size=(128, 128), mode='bicubic', align_corners=False)\n",
    "        \n",
    "        # Normalize the sample\n",
    "        sample = min_max_normalize(sample) \n",
    "        \n",
    "        sample = sample.squeeze(0)  # Removes batch dimension\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        # label data\n",
    "        sample_label = data[\"label\"] \n",
    "        # Add channel dimension\n",
    "        sample_label = np.expand_dims(sample_label, axis=0)\n",
    "\n",
    "        # Add batch dimension and resize to 256x256 using bicubic interpolation\n",
    "        sample_label = torch.tensor(sample_label, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "        sample_label = F.interpolate(sample_label, size=(128, 128), mode='bicubic', align_corners=False)\n",
    "        \n",
    "        # Normalize the sample\n",
    "        sample_label = min_max_normalize(sample_label)  \n",
    "\n",
    "        sample_label = sample_label.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "        if self.transform:\n",
    "            sample_label = self.transform(sample_label)\n",
    "            \n",
    "        return sample.clone().detach().to(dtype=torch.float32), sample_label.clone().detach().to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83949c5b-61d6-4660-91d6-53815f8b19b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_abnormal_labels = NPZDataset2(root_dir=\"abnormal_slices\", indices=list(range(1600)))\n",
    "test_data_abnormal_loader = DataLoader(test_data_abnormal_labels, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ff14a-30be-4777-bfc5-911ff6393c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real anomalies dice score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603bd140-6561-4b81-a440-6123645bd292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_abnormal_loader should yield abnormal images and corresponding ground truth mask \n",
    "\n",
    "dice_scores = []  # List to store dice scores for all test samples\n",
    "\n",
    "# Loop over the test dataset\n",
    "for batch in test_data_abnormal_loader:\n",
    "    images, ground_truth_masks = batch  # assuming each batch yields images and corresponding ground truth masks\n",
    "    \n",
    "    # Loop over the batch\n",
    "    for i in range(len(images)):\n",
    "        image = images[i].to(device)  # send image to device\n",
    "        ground_truth_mask = ground_truth_masks[i].to(device)  # send ground truth mask to device\n",
    "        \n",
    "        # Calculate predicted_mask for the image based on log-likelihood\n",
    "        log_likelihood = inferer.get_likelihood(\n",
    "            inputs=image[None, ...].to(device),\n",
    "            vqvae_model=vqvae_model,\n",
    "            transformer_model=transformer_model,\n",
    "            ordering=ordering)\n",
    "        likelihood = torch.exp(log_likelihood)\n",
    "        mask = log_likelihood.cpu()[0, ...] < torch.quantile(log_likelihood, 0.04).item()\n",
    "        # Further mask with the healing mask\n",
    "        resizer = torch.nn.Upsample(size=(128, 128), mode=\"nearest\")\n",
    "        predicted_mask = resizer(mask[None, None, ...].float()).int().squeeze().to(device)\n",
    "        \n",
    "        # Now calculate the dice score for this image\n",
    "        score = dice_score(predicted_mask, ground_truth_mask)\n",
    "        dice_scores.append(score.item())\n",
    "\n",
    "# Now calculate the average dice score for the entire test dataset\n",
    "average_dice_score = sum(dice_scores) / len(dice_scores)\n",
    "\n",
    "print(f'Average Dice Score on Test Dataset: {average_dice_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585634d-da30-4bf8-828b-30a4a41f0636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c19d2a-6ba5-4712-845e-cd5f42cf3fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb2247-68de-406a-9886-d550721aa0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aeac2b-ede1-4bf2-8ef2-19b701f1db0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45992684-db8a-41e7-ab98-b70f693f6821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87364f-0319-4ef6-86ac-5e727fab1a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_image_seg",
   "language": "python",
   "name": "med_image_seg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
